{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123c315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "    \n",
    "def enhancer_collate_fn(batch):\n",
    "    \"\"\"This pads the data to the max sequence length in the batch to reduce memory usage during training\n",
    "        Returns the sequences as padded, mask tensor for attention parameter, and labels\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)  # batch is list of (tokens, target)\n",
    "\n",
    "    # First normalize all seqs to (L_i, embed_dim)\n",
    "    arrays = []\n",
    "    lengths = []\n",
    "    for seq in sequences:\n",
    "        arr = np.array(seq, dtype=np.float32)\n",
    "        if arr.ndim == 1:\n",
    "            # Treat as a single-token sequence: (embed_dim,) -> (1, embed_dim)\n",
    "            arr = arr[None, :]\n",
    "        arrays.append(arr)\n",
    "        lengths.append(arr.shape[0])\n",
    "\n",
    "    max_len = max(lengths)\n",
    "    embed_dim = arrays[0].shape[1]\n",
    "\n",
    "    padded = np.zeros((len(arrays), max_len, embed_dim), dtype=np.float32)\n",
    "    mask = np.zeros((len(arrays), max_len), dtype=np.bool_)\n",
    "\n",
    "    for i, arr in enumerate(arrays):\n",
    "        L = arr.shape[0]\n",
    "        padded[i, :L, :] = arr\n",
    "        mask[i, :L] = 1  # 1 for real token, 0 for padding\n",
    "\n",
    "    padded = torch.from_numpy(padded)         # (batch, max_len, embed_dim)\n",
    "    mask = torch.from_numpy(mask)             # (batch, max_len)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)  # (batch,)\n",
    "\n",
    "    return padded, mask, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333cc14",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abc023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/k_vp1md94sz2dr40dtrs6ln80000gn/T/ipykernel_45604/3723224271.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  arr = np.array(seq, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] Step [100] - Loss: 0.6680\n",
      "Epoch [1/2] Step [200] - Loss: 0.6342\n",
      "Epoch [1/2] Step [300] - Loss: 0.6288\n",
      "Epoch [1/2] Step [400] - Loss: 0.6319\n",
      "Epoch [1/2] Step [500] - Loss: 0.6386\n",
      "Epoch [1/2] Step [600] - Loss: 0.6145\n",
      "Epoch [1/2] Step [700] - Loss: 0.6338\n",
      "Epoch [1/2] Step [800] - Loss: 0.6187\n",
      "Epoch [1/2] Step [900] - Loss: 0.6330\n",
      "Epoch [1/2] Step [1000] - Loss: 0.6300\n",
      "Epoch [1/2] Step [1100] - Loss: 0.6292\n",
      "Epoch [1/2] Step [1200] - Loss: 0.6142\n",
      "Epoch [1/2] Step [1300] - Loss: 0.6263\n",
      "Epoch [1/2] Step [1400] - Loss: 0.6363\n",
      "Epoch [1/2] Step [1500] - Loss: 0.6372\n",
      "Epoch [1/2] Step [1600] - Loss: 0.6290\n",
      "Epoch [1/2] Step [1700] - Loss: 0.6148\n",
      "Epoch [1/2] Step [1800] - Loss: 0.5969\n",
      "Epoch [1/2] Step [1900] - Loss: 0.6196\n",
      "Epoch [1/2] Step [2000] - Loss: 0.6105\n",
      "Epoch [1/2] Step [2100] - Loss: 0.6069\n",
      "Epoch [1/2] Step [2200] - Loss: 0.6241\n",
      "Epoch [1/2] Step [2300] - Loss: 0.6135\n",
      "Epoch [1/2] Step [2400] - Loss: 0.6101\n",
      "Epoch [1/2] Step [2500] - Loss: 0.6230\n",
      "Epoch [1/2] Step [2600] - Loss: 0.6278\n",
      "Epoch [1/2] Step [2700] - Loss: 0.6028\n",
      "Epoch [1/2] Step [2800] - Loss: 0.6082\n",
      "Epoch [1/2] Step [2900] - Loss: 0.6037\n",
      "Epoch [1/2] Step [3000] - Loss: 0.6082\n",
      "Epoch [1/2] Step [3100] - Loss: 0.5904\n",
      "Epoch [1/2] Step [3200] - Loss: 0.6002\n",
      "Epoch [1/2] Step [3300] - Loss: 0.6150\n",
      "Epoch [1/2] Step [3400] - Loss: 0.6241\n",
      "Epoch [1/2] Step [3500] - Loss: 0.6269\n",
      "Epoch [1/2] Step [3600] - Loss: 0.5970\n",
      "Epoch [1/2] Step [3700] - Loss: 0.5978\n",
      "Epoch [1/2] Step [3800] - Loss: 0.5893\n",
      "Epoch [1/2] Step [3900] - Loss: 0.6098\n",
      "Epoch [1/2] Step [4000] - Loss: 0.6160\n",
      "Epoch [1/2] Step [4100] - Loss: 0.5988\n",
      "Epoch [1/2] Step [4200] - Loss: 0.5815\n",
      "Epoch [1/2] Step [4300] - Loss: 0.5985\n",
      "Epoch [1/2] Step [4400] - Loss: 0.5877\n",
      "Epoch [1/2] Step [4500] - Loss: 0.5886\n",
      "Epoch [1/2] Step [4600] - Loss: 0.6016\n",
      "Epoch [1/2] Step [4700] - Loss: 0.6072\n",
      "Epoch [1/2] Step [4800] - Loss: 0.6036\n",
      "Epoch [1/2] Step [4900] - Loss: 0.5818\n",
      "Epoch [1/2] Step [5000] - Loss: 0.5809\n",
      "Epoch [1/2] Step [5100] - Loss: 0.5979\n",
      "Epoch [1/2] Step [5200] - Loss: 0.5848\n",
      "Epoch [1/2] Step [5300] - Loss: 0.5953\n",
      "Epoch [1/2] Step [5400] - Loss: 0.5761\n",
      "Epoch [1/2] Step [5500] - Loss: 0.6096\n",
      "Epoch [1/2] Step [5600] - Loss: 0.5851\n",
      "Epoch [1/2] Step [5700] - Loss: 0.6126\n",
      "Epoch [1/2] Step [5800] - Loss: 0.5815\n",
      "Epoch [1/2] Step [5900] - Loss: 0.5764\n",
      "Epoch [1/2] Step [6000] - Loss: 0.5813\n",
      "Epoch [1/2] Step [6100] - Loss: 0.5855\n",
      "Epoch [1/2] Step [6200] - Loss: 0.5861\n",
      "Epoch [1/2] Step [6300] - Loss: 0.5862\n",
      "Epoch [1/2] Step [6400] - Loss: 0.5810\n",
      "Epoch [1/2] Step [6500] - Loss: 0.6018\n",
      "Epoch [1/2] Step [6600] - Loss: 0.5881\n",
      "Epoch [1/2] Step [6700] - Loss: 0.6005\n",
      "Epoch [1/2] Step [6800] - Loss: 0.5604\n",
      "Epoch [1/2] Step [6900] - Loss: 0.5595\n",
      "Epoch [1/2] Step [7000] - Loss: 0.5750\n",
      "Epoch [1/2] Step [7100] - Loss: 0.5969\n",
      "Epoch [1/2] Step [7200] - Loss: 0.5798\n",
      "Epoch [1/2] Step [7300] - Loss: 0.5797\n",
      "Epoch [1/2] Step [7400] - Loss: 0.5771\n",
      "Epoch [2/2] Step [100] - Loss: 0.5738\n",
      "Epoch [2/2] Step [200] - Loss: 0.5906\n",
      "Epoch [2/2] Step [300] - Loss: 0.5822\n",
      "Epoch [2/2] Step [400] - Loss: 0.5931\n",
      "Epoch [2/2] Step [500] - Loss: 0.5674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     49\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m optimizer.step()\n\u001b[32m     53\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from EnhancerDataset import EnhancerDataset\n",
    "from EnhancerAttention import EnhancerAttention\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "train_dataset = EnhancerDataset(data_type=\"train\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=enhancer_collate_fn,\n",
    ")\n",
    "\n",
    "# Model, loss, optimizer\n",
    "embed_size = 100  # dna2vec dim\n",
    "model = EnhancerAttention(embed_size=embed_size).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "epochs = \n",
    "print_every = 100  # steps\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (inputs, masks, labels) in enumerate(train_loader, start=1):\n",
    "        # Move to device\n",
    "        inputs = inputs.to(device)           # (batch, N, embed_dim)\n",
    "        masks = masks.to(device) if masks is not None else None\n",
    "        labels = labels.to(device).float()   # (batch,)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(inputs, masks)        # (batch,)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            avg_loss = running_loss / print_every\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Step [{step}] - Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd934c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Full training set size: 119040\n",
      "Using subset size: 11904 (10.0% of data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/k_vp1md94sz2dr40dtrs6ln80000gn/T/ipykernel_45604/3723224271.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  arr = np.array(seq, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Step [100] - Loss: 0.0881 - RAM: 275.0 MB\n",
      "Epoch [1/1] Step [200] - Loss: 0.0009 - RAM: 147.8 MB\n",
      "Epoch [1/1] Step [300] - Loss: 0.0004 - RAM: 160.8 MB\n",
      "Epoch [1/1] Step [400] - Loss: 0.0002 - RAM: 136.0 MB\n",
      "Epoch [1/1] Step [500] - Loss: 0.0002 - RAM: 222.3 MB\n",
      "Epoch [1/1] Step [600] - Loss: 0.0001 - RAM: 205.2 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     75\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m optimizer.step()\n\u001b[32m     79\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from EnhancerDataset import EnhancerDataset\n",
    "from EnhancerAttention import EnhancerAttention\n",
    "\n",
    "# for memory usage on CPU\n",
    "try:\n",
    "    import psutil\n",
    "    USE_PSUTIL = True\n",
    "except ImportError:\n",
    "    USE_PSUTIL = False\n",
    "\n",
    "# ---------------------------\n",
    "# Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 10% Subset of dataset\n",
    "full_train_dataset = EnhancerDataset(data_type=\"train\")\n",
    "full_size = len(full_train_dataset)\n",
    "subset_fraction = 0.10\n",
    "subset_size = int(full_size * subset_fraction)\n",
    "\n",
    "# take the first 10% of indices\n",
    "subset_indices = list(range(subset_size))\n",
    "train_dataset = Subset(full_train_dataset, subset_indices)\n",
    "\n",
    "print(f\"Full training set size: {full_size}\")\n",
    "print(f\"Using subset size: {subset_size} ({subset_fraction*100:.1f}% of data)\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=enhancer_collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "embed_size = 100  # dna2vec dim\n",
    "model = EnhancerAttention(embed_size=embed_size).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop (benchmark run)\n",
    "epochs = 1 # for preliminary benchmarking\n",
    "print_every = 100 # steps\n",
    "\n",
    "start_time = time.time()\n",
    "if USE_PSUTIL:\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (inputs, masks, labels) in enumerate(train_loader, start=1):\n",
    "        inputs = inputs.to(device) # (batch, N, embed_dim)\n",
    "        masks = masks.to(device) if masks is not None else None\n",
    "        labels = labels.to(device).float() # (batch,)\n",
    "\n",
    "        # Forward\n",
    "        logits = model(inputs, masks) # (batch,)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            avg_loss = running_loss / print_every\n",
    "            if USE_PSUTIL:\n",
    "                mem_mb = process.memory_info().rss / (1024 ** 2)\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{epochs}] Step [{step}] \"\n",
    "                    f\"- Loss: {avg_loss:.4f} - RAM: {mem_mb:.1f} MB\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] Step [{step}] - Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Benchmark training complete in {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24799097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
